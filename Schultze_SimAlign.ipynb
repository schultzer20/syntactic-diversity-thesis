{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "opXm81gwbR-4",
    "outputId": "6dadd107-84d1-47ee-88c5-5b8ecf976c87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simalign\n",
      "  Cloning https://github.com/cisnlp/simalign.git (to revision master) to /tmp/pip-install-3fjry3q6/simalign_dfc88ea1e9dd466da9c9f16f5f27cc4b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/cisnlp/simalign.git /tmp/pip-install-3fjry3q6/simalign_dfc88ea1e9dd466da9c9f16f5f27cc4b\n",
      "  Resolved https://github.com/cisnlp/simalign.git to commit ca1932a3655e54880f6f92b73c59100c171c31da\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from simalign) (2.0.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from simalign) (2.6.0+cu124)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from simalign) (1.15.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from simalign) (4.53.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from simalign) (2024.11.6)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from simalign) (3.5)\n",
      "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.11/dist-packages (from simalign) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->simalign) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->simalign) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->simalign) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->simalign) (4.14.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->simalign) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->simalign) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->simalign)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->simalign)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->simalign)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->simalign)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->simalign)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->simalign)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->simalign)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->simalign)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->simalign)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->simalign) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->simalign) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->simalign) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->simalign)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->simalign) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->simalign) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->simalign) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->simalign) (0.33.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->simalign) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->simalign) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->simalign) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->simalign) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->simalign) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->simalign) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->simalign) (1.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->simalign) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->simalign) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->simalign) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->simalign) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->simalign) (2025.7.14)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m249.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m195.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m254.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m239.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m262.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m247.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m138.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m215.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: simalign\n",
      "  Building wheel for simalign (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for simalign: filename=simalign-0.4-py3-none-any.whl size=8110 sha256=52f6d0caa164592caa7f58d5715ba66635c0322ee08e98a016024aecfa311261\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-viawqazx/wheels/7e/4d/7c/204070a551ff528561d5c065436e7e9f3172cbf12498d15043\n",
      "Successfully built simalign\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, simalign\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 simalign-0.4\n",
      "Collecting stanza\n",
      "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (5.29.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.5)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.14.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.7.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
      "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.14.1 stanza-1.10.1\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries\n",
    "!pip install --upgrade --no-cache-dir git+https://github.com/cisnlp/simalign.git@master#egg=simalign\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "y9pgZ3kImlJr",
    "outputId": "6b5cbf62-2ceb-4635-a94e-28681e8c0ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUL18c84moTZ"
   },
   "outputs": [],
   "source": [
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gd39snsHde6j"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I74RtcmbUzv"
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "from simalign import SentenceAligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400,
     "referenced_widgets": [
      "49d350d988ae4501ab0e4c44ddb12d34",
      "35c7d6a112a04bb2bc517058f57faa4b",
      "711646fcbe72459d9f52668228fd249c",
      "8d8f4b0aa99f4b269f031ec7a2bb3878",
      "fa1e5fc7df31455e880e09796407944c",
      "31d6bfbd07b2481e88ee14a8606cd1c9",
      "fa522029c3db4b4bb347cd06a22ba4a1",
      "d514636322ed43788900fad047c44587",
      "aa6e9073f852412da81796c7c304eec7",
      "c29c6304c24c40b2879950a5f5a2cf3d",
      "0fdf412590a04c23ac4ec85d3b825de6",
      "84fe4a4ea72343959f92bc4801513678",
      "bab0afd8b3174abbb438066e21dd419b",
      "66491040529543e3ad0d0ae27d073b8d",
      "8ae5e2af0f234dabbdd0b44cd042c7d1",
      "165c4c0eae594864bbcdeb9964f6b543",
      "f3bf717f0e2c4c969a31b609bada4d33",
      "bfd3de18cf1248e796ec5c495b65c9fb",
      "720e2c242a1c4ef48b98f9bebd5fb4ff",
      "6f2c19e449834a6da3b05b80a806271b",
      "d6be8e663fbe45a0a9bd9d5c39c9211e",
      "5a73b4ef8161401abca0d4b6e88e415d",
      "b4c5726570aa49f1b6720143273c3d79",
      "115a78ff15b04e7d8616c3c25629c0cc",
      "baa9eff96cd34a6a90abbc1afa5686e1",
      "8bce5a963e594b8985aaef1d6205bfdc",
      "1c6f1ea01e9641a6bcb65dac1a0f9278",
      "a0de6d355ccd4fe29c5fa85f96799666",
      "3cf885b6ceb0441fbce7f862ace61b3d",
      "d1945906370f4d7d8428346d81bdf934",
      "564296b8b1a741e4853aca1eefa4b39e",
      "43bdb47e89ff476b912de7349500aee2",
      "926f5aab8eea4de185313c369e84619a",
      "8ec8809b95714cf58863c2f0097a417f",
      "cb2da729b23940c18dba1f3c3ccb74a3",
      "3b1fdcf590ec48978312aa1c45db7b3d",
      "8927e0e0e7474dd78ead022b9e51546d",
      "fd9c5443abe94874a7bd89e1bb1d9d1d",
      "463153a2ee504fdebead2c21fec957fc",
      "30018c40a60f46c4b2cb00164af93850",
      "a1e6a28d55f84c6fbcb9f139cb832b10",
      "648be4b45246465ca99642f161248927",
      "a7248d8d6d0840d0b3f897face100309",
      "f539cb1825b04a4f97cdb1dad9267bc8"
     ]
    },
    "collapsed": true,
    "id": "MQsic0F7bXh_",
    "outputId": "890382bf-ea12-471e-e240-5918f17bf7ef"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d350d988ae4501ab0e4c44ddb12d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "INFO:stanza:Downloading default packages for language: de (German) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fe4a4ea72343959f92bc4801513678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/default.zip:   0%|          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/de/default.zip\n",
      "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c5726570aa49f1b6720143273c3d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "INFO:stanza:Downloading default packages for language: pt (Portuguese) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec8809b95714cf58863c2f0097a417f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/default.zip:   0%|          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/pt/default.zip\n",
      "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "# Download the German language model (once per session)\n",
    "stanza.download(\"de\")\n",
    "stanza.download(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "04e167fab7b24a26893af45472b221a2",
      "9056f57a374f47928c1e945ea720b24d",
      "8d6cf3fc07d04fa6ad604c4903ae5dcd",
      "4468d7edaeac4dc1bf97322b781c65b8",
      "e71806883d364e31aeee4a97ee40cb0b",
      "b6797fc763354a47a286c394abd276db",
      "a795196273574e119bf756fe9acd3fce",
      "ad16c35920cc483ba662139bedbf2a18",
      "e73ebc2bef2d4cb784c07949b937a037",
      "75211d1b7faa45d4a886addbbacb4c76",
      "31e6154cc43343aeaaf9d104a589a6ec"
     ]
    },
    "collapsed": true,
    "id": "8AyR-dtvbdAg",
    "outputId": "549ad59c-6462-4903-ffc0-61543ecd1a30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e167fab7b24a26893af45472b221a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "WARNING:stanza:Language de package default expects mwt, which has been added\n",
      "INFO:stanza:Loading these models for language: de (German):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | spmrl_charlm      |\n",
      "| depparse     | combined_charlm   |\n",
      "====================================\n",
      "\n",
      "INFO:stanza:Using device: cpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: mwt\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Loading: lemma\n",
      "INFO:stanza:Loading: constituency\n",
      "INFO:stanza:Loading: depparse\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Load the German tokenizer\n",
    "nlp_de = stanza.Pipeline(lang=\"de\", processors=\"tokenize,pos,lemma,depparse,constituency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "72652f44873e4ad0bb8f77b04dcbdd9d",
      "e679f72dc33947e6a07a41b2337752dd",
      "e5aee54265c04768bc7e1abf75180779",
      "9b96af7348a74cbab85fc16baff7c6a3",
      "2c04245c098042189b369429271d81a9",
      "c01f781437d74c2ab435032f60cc7e9a",
      "743688b154a34cf0af460229c44f2f67",
      "730cfcfb7aff4a6e9efa1b74b6fa00cc",
      "49c6ace3f63643b9a2f47fe684cde075",
      "7cb8e3e8b3284b78a18f3a33093c81d7",
      "74fd4f904d18434ca1de45088319f756"
     ]
    },
    "collapsed": true,
    "id": "YpEhs9qjgYir",
    "outputId": "9e43c242-e4a3-4eaa-cf97-43554e78a196"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72652f44873e4ad0bb8f77b04dcbdd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "WARNING:stanza:Language pt package default expects mwt, which has been added\n",
      "INFO:stanza:Loading these models for language: pt (Portuguese):\n",
      "==================================\n",
      "| Processor    | Package         |\n",
      "----------------------------------\n",
      "| tokenize     | bosque          |\n",
      "| mwt          | bosque          |\n",
      "| pos          | bosque_charlm   |\n",
      "| lemma        | bosque_nocharlm |\n",
      "| constituency | cintil_charlm   |\n",
      "| depparse     | bosque_charlm   |\n",
      "==================================\n",
      "\n",
      "INFO:stanza:Using device: cpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: mwt\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Loading: lemma\n",
      "INFO:stanza:Loading: constituency\n",
      "INFO:stanza:Loading: depparse\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Load the Portuguese tokenizer\n",
    "nlp_pt = stanza.Pipeline(lang='pt', processors='tokenize,pos,lemma,depparse,constituency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340,
     "referenced_widgets": [
      "6a48f582a40e4ea09e571299cb38bb8d",
      "1a25f0510577443cbc3271b2479c0e3a",
      "4d4f555fb30042468d80eae7d525f49c",
      "749915cea11f4562b51f1fb2dbec059c",
      "77c614580e1246a59e58643fd2e7574d",
      "5368e5bc0b0d41a7a5a529c5e2383496",
      "954ecb3c21014ec0bbdac527d8fd21ef",
      "0aa440897b814434bae080414ea7c62a",
      "c16757b6194848e8a3c9ff65b27de210",
      "61b10366e56e4b6b94df360a64328ad2",
      "5fcbd89eec9d4a93b3fbef779a3a46f8",
      "32831a7c40654adba832cdd3bd58b446",
      "ec21b58d582e4851a002be8b53e42c07",
      "255513e93dc440ea858152837adb48b2",
      "8f1e1dca24b14f0eb52352d31f9eb313",
      "55c35bfdb8624a84adbc023b4e6b700b",
      "f0fff4fb96fe49288e2d74f322ddd733",
      "da2d3d3b7ed34475af0f88dad3980079",
      "325fe8357d0945c18b2559dbd5f73f56",
      "2545fd63a7a342bcbfd9a3850b3ddab8",
      "d6c39e5c4c074fda92f55f20989a25af",
      "345fe29b6e7e4c10a5f0f5ca0f304ef3",
      "a343149a81e84b0cadcd967d488adfb7",
      "76b120daa5e24975b3d84a30f9bda338",
      "c2621fafb6d6434f9a56f26379795095",
      "aedc4c2f5a2b415f8d04940df0054b0b",
      "fd26510a067a4553927024b25683d706",
      "01ea46cd44b94fc391192324bb926446",
      "1df6f3336e29495ebc498984b15ea228",
      "f8ded2235f3c418396042c433b0969ba",
      "a273c27a04cf4c1c977714aa2abc4826",
      "f530de07f99a47a1b8d26d88b9c7f0cc",
      "3c51acd1d6074ef4b6cb723c545a2e02",
      "1cc88410088f4666b632605f91215460",
      "6b21731961464661a1bcbf09365f7aec",
      "aa87ded888524fdabfb633c9a5d98b71",
      "fb6c00304ba84179bd59caa4fda66b2a",
      "a9976e3a9abc4b4ebfc629c26f9996a1",
      "651e9cd222aa4b43a8df4ffb8c579e25",
      "e320c7153f5946e3918c396c036ac700",
      "e89dd97fbb0f48088b6c2bf71d4b6de3",
      "03f37ffae0844d69805ee38e23d6a6ad",
      "1558afb9e16847fab8348387972159cf",
      "9bafa10ebf9c41308a1d5a34f6c79770",
      "a8397417587e406791b4854eb7b04c39",
      "02968700c48e49099edf278357afea51",
      "34d96609ca4341af90288986e89a34e3",
      "b055643eb8b44efe88a103215536a837",
      "7937cd8e84ed49b9a44a186b895166b3",
      "b50024e08d304de9ae9b8a40823cb3bd",
      "bdd96cd11d3843fc9c8b3bcdc3ff6648",
      "e7bebe0fa5a048bab8dae41bf6f2b544",
      "69a326c71ba64d0ab8fb9f28ce37715c",
      "45fb822aa31549079126d8965f4e0d2d",
      "fc19e83f78704bb296bf3ffc4154ed09"
     ]
    },
    "collapsed": true,
    "id": "3MkTkn7BbfSN",
    "outputId": "c6eb03fa-03a3-4f23-e5a7-a0cb3d2202cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a48f582a40e4ea09e571299cb38bb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32831a7c40654adba832cdd3bd58b446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a343149a81e84b0cadcd967d488adfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc88410088f4666b632605f91215460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8397417587e406791b4854eb7b04c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 06:57:53,696 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n",
      "INFO:simalign.simalign:Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "# Initialize SimAlign with \"i\" for itermax (or \"mai\" for all strategies)\n",
    "aligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhE1Alz_bjdi",
    "outputId": "c7834cd0-24cb-43d0-b108-a4acacbb829d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignments: [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Define tokenized German sentences (lists of words)\n",
    "src = [\"Das\", \"ist\", \"ein\", \"Test\", \".\"]\n",
    "tgt = [\"Dies\", \"ist\", \"ein\", \"Versuch\", \".\"]\n",
    "\n",
    "# Get alignment using itermax\n",
    "result = aligner.get_word_aligns(src, tgt)\n",
    "print(\"Alignments:\", result[\"itermax\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sW6aCxf6jr6V"
   },
   "outputs": [],
   "source": [
    "# Easy lookup table\n",
    "PIPELINES = {\"de\": nlp_de, \"pt\": nlp_pt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf0UaaKScElo"
   },
   "source": [
    "# Align Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9D4ijbALeEV7"
   },
   "outputs": [],
   "source": [
    "# Load TSV file\n",
    "def load_tsv(file_path):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        return [tuple(row) for row in csv.reader(f, delimiter=\"\\t\") if len(row) == 2]\n",
    "\n",
    "# Tokenize with stanza\n",
    "def tokenize_stanza(text, pipeline):\n",
    "    doc = pipeline(text)\n",
    "    return [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "def get_constituency_pretty(s):\n",
    "    \"\"\"\n",
    "    Safely get a pretty-printed constituency tree string.\n",
    "    If s.constituency is string, parse and pretty-print,\n",
    "    else return string representation.\n",
    "    \"\"\"\n",
    "    tree_obj = s.constituency\n",
    "    if tree_obj is None:\n",
    "        return None\n",
    "    if isinstance(tree_obj, str):\n",
    "        try:\n",
    "            tree = Tree.fromstring(tree_obj)\n",
    "            return tree.pformat()\n",
    "        except Exception:\n",
    "            # parsing failed, return raw string\n",
    "            return tree_obj\n",
    "    elif isinstance(tree_obj, Tree):\n",
    "        return tree_obj.pformat()\n",
    "    else:\n",
    "        return str(tree_obj)\n",
    "\n",
    "def align_file_with_deprels(tsv_path, src_pipeline, tgt_pipeline, aligner):\n",
    "    pairs = load_tsv(tsv_path)\n",
    "    results = []\n",
    "\n",
    "    for i, (src, tgt) in enumerate(pairs):\n",
    "        src_doc = src_pipeline(src)\n",
    "        tgt_doc = tgt_pipeline(tgt)\n",
    "\n",
    "        def extract_info(doc):\n",
    "            tokens     = [w.text for s in doc.sentences for w in s.words]\n",
    "            deprels    = [w.deprel for s in doc.sentences for w in s.words]\n",
    "            pos_tags   = [w.upos for s in doc.sentences for w in s.words]\n",
    "            lemmas     = [w.lemma for s in doc.sentences for w in s.words]\n",
    "            heads_1_based = [w.head for s in doc.sentences for w in s.words]\n",
    "            heads_0_based = [h - 1 if h > 0 else -1 for h in heads_1_based]\n",
    "            constituents = [get_constituency_pretty(s) for s in doc.sentences if s.constituency]\n",
    "            return {\n",
    "                \"tokens\": tokens,\n",
    "                \"deprels\": deprels,\n",
    "                \"pos\": pos_tags,\n",
    "                \"lemmas\": lemmas,\n",
    "                \"heads_1_based\": heads_1_based,\n",
    "                \"heads_0_based\": heads_0_based,\n",
    "                \"constituents\": constituents,\n",
    "            }\n",
    "\n",
    "        src_info = extract_info(src_doc)\n",
    "        tgt_info = extract_info(tgt_doc)\n",
    "\n",
    "        alignment = aligner.get_word_aligns(src_info[\"tokens\"], tgt_info[\"tokens\"])\n",
    "\n",
    "        results.append({\n",
    "            \"index\": i,\n",
    "            \"src\": src_info,\n",
    "            \"tgt\": tgt_info,\n",
    "            \"alignments\": {\n",
    "                \"mwmf\": alignment[\"mwmf\"],\n",
    "                \"inter\": alignment[\"inter\"],\n",
    "                \"itermax\": alignment[\"itermax\"]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def guess_languages(tsv_name):\n",
    "    \"\"\"\n",
    "    Very simple heuristic:\n",
    "    - If the file name contains '_mt_ht' or '_mt_pemt', both columns are German ('de').\n",
    "    - Otherwise we assume source = Portuguese ('pt'), target = German ('de').\n",
    "    Adjust to match your real naming scheme!\n",
    "    \"\"\"\n",
    "    name = tsv_name.lower()\n",
    "    if \"_mt_ht\" in name or \"_mt_pemt\" in name:\n",
    "        return \"de\", \"de\"\n",
    "    else:                               # e.g. st_ht, st_pemt\n",
    "        return \"pt\", \"de\"\n",
    "\n",
    "# Save as JSON\n",
    "def save_alignments(results, json_path):\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLhgEwq0kc-W",
    "outputId": "ef5549e0-d82d-4a37-a256-2ebd8976f309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing qualquer-serve_mt_ht.tsv  |  src=de  tgt=de\n",
      "Saved alignment to /content/qualquer-serve_mt_ht.align.json\n"
     ]
    }
   ],
   "source": [
    "# Folder where the TSV files are\n",
    "# folder = \"/content\"\n",
    "# filename = \"qualquer-serve_mt_ht.tsv\"   # or \"qualquer-serve_st_ht.tsv\", …\n",
    "\n",
    "# tsv_path  = os.path.join(folder, filename)\n",
    "# json_path = os.path.join(folder, filename.replace(\".tsv\", \".align.json\"))\n",
    "\n",
    "# # ① decide languages\n",
    "# src_lang, tgt_lang = guess_languages(filename)\n",
    "\n",
    "# # ② fetch the right pipelines\n",
    "# src_pl  = PIPELINES[src_lang]\n",
    "# tgt_pl  = PIPELINES[tgt_lang]\n",
    "\n",
    "# print(f\"Processing {filename}  |  src={src_lang}  tgt={tgt_lang}\")\n",
    "\n",
    "# # ③ align and save\n",
    "# aligned = align_file_with_deprels(tsv_path, src_pl, tgt_pl, aligner)\n",
    "# save_alignments(aligned, json_path)\n",
    "\n",
    "# print(f\"Saved alignment to {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_1oEQeJpF_W"
   },
   "source": [
    "# Process TSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHwdpXR-pIBw",
    "outputId": "db9ddbc0-00f2-422c-deff-3fdd78d1d870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing o-pavilhão-e-a-pinel_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/o-pavilhão-e-a-pinel_st_mt.align.json\n",
      "Processing cada-raça-tem-um-calino_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/cada-raça-tem-um-calino_st_mt.align.json\n",
      "Processing a-volta_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/a-volta_st_mt.align.json\n",
      "Processing uma-outra_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/uma-outra_st_mt.align.json\n",
      "Processing não-se-zanguem_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/não-se-zanguem_st_mt.align.json\n",
      "Processing efeitos-da-lei-valetudinária_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/efeitos-da-lei-valetudinária_st_mt.align.json\n",
      "Processing uma-anedota_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/uma-anedota_st_mt.align.json\n",
      "Processing o-dicionário_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/o-dicionário_st_mt.align.json\n",
      "Processing linhas-de-tiro_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/linhas-de-tiro_st_mt.align.json\n",
      "Processing não-as-matem_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/não-as-matem_st_mt.align.json\n",
      "Processing o-nosso-feminism_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/o-nosso-feminism_st_mt.align.json\n",
      "Processing a-cartomante_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/a-cartomante_st_mt.align.json\n",
      "Processing a-questão-dos-telefones_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/a-questão-dos-telefones_st_mt.align.json\n",
      "Processing velhos-apedidos-e-velho-anúncios_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/velhos-apedidos-e-velho-anúncios_st_mt.align.json\n",
      "Processing as-vaporosas_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/as-vaporosas_st_mt.align.json\n",
      "Processing fala-o-corvo_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/fala-o-corvo_st_mt.align.json\n",
      "Processing qualquer-serve_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/qualquer-serve_st_mt.align.json\n",
      "Processing modas-femininas-e-outras_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/modas-femininas-e-outras_st_mt.align.json\n",
      "Processing exemplo-a-imitar_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/exemplo-a-imitar_st_mt.align.json\n",
      "Processing uma-partida-de-football_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/uma-partida-de-football_st_mt.align.json\n",
      "Processing a-lei_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/a-lei_st_mt.align.json\n",
      "Processing papel-moeda_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/papel-moeda_st_mt.align.json\n",
      "Processing no-primor-da-elegância_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/no-primor-da-elegância_st_mt.align.json\n",
      "Processing uma-lembrança_st_mt.tsv | src=pt tgt=de\n",
      "Saved alignment to /content/uma-lembrança_st_mt.align.json\n"
     ]
    }
   ],
   "source": [
    "# Folder where the TSV files are\n",
    "folder = \"/content\"\n",
    "\n",
    "# Prepare pipelines once\n",
    "src_de = nlp_de\n",
    "tgt_de = nlp_de\n",
    "src_pt = nlp_pt\n",
    "tgt_pt = nlp_de  # always German as target for pt-de pairs\n",
    "\n",
    "# Process all TSV files in folder\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".tsv\"):\n",
    "        tsv_path = os.path.join(folder, filename)\n",
    "        json_path = os.path.join(folder, filename.replace(\".tsv\", \".align.json\"))\n",
    "\n",
    "        # Guess languages from filename (adjust this function as needed)\n",
    "        src_lang, tgt_lang = guess_languages(filename)\n",
    "\n",
    "        # Choose pipelines accordingly\n",
    "        if src_lang == \"de\" and tgt_lang == \"de\":\n",
    "            src_pipeline = src_de\n",
    "            tgt_pipeline = tgt_de\n",
    "        elif src_lang == \"pt\" and tgt_lang == \"de\":\n",
    "            src_pipeline = src_pt\n",
    "            tgt_pipeline = tgt_de\n",
    "        else:\n",
    "            print(f\"Skipping {filename} due to unknown language pair ({src_lang}-{tgt_lang})\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {filename} | src={src_lang} tgt={tgt_lang}\")\n",
    "\n",
    "        aligned = align_file_with_deprels(tsv_path, src_pipeline, tgt_pipeline, aligner)\n",
    "        save_alignments(aligned, json_path)\n",
    "\n",
    "        print(f\"Saved alignment to {json_path}\")\n",
    "\n",
    "# Zip all JSON files\n",
    "#zip_name = \"alignments_json.zip\"\n",
    "#shutil.make_archive(zip_name.replace(\".zip\", \"\"), 'zip', folder)\n",
    "\n",
    "#from google.colab import files\n",
    "#files.download(zip_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "tTc3SDKbCLr6",
    "outputId": "5f0f98ff-e229-41c7-e98c-7e09224b657e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading a-lei_st_mt.align.json\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_dab3ac86-1af1-421f-a346-517f1ed7d5f5\", \"a-lei_st_mt.align.json\", 128474)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading papel-moeda_st_mt.align.json\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_1e690702-91a7-4e90-aff4-539cab97a02f\", \"papel-moeda_st_mt.align.json\", 172287)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading não-as-matem_st_mt.align.json\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_c775bc91-16c6-472c-89c3-1ab5862b70d4\", \"n\\u00e3o-as-matem_st_mt.align.json\", 195232)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading uma-anedota_st_mt.align.json\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_e8894cc8-c586-4b1c-bf95-c3440bd45ded\", \"uma-anedota_st_mt.align.json\", 100915)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading uma-partida-de-football_st_mt.align.json\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_860d599c-9b09-4b06-8bc9-db7bfc14f874\", \"uma-partida-de-football_st_mt.align.json\", 125227)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading o-nosso-feminism_st_mt.align.json\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_f3aa4f9f-3a56-400a-b207-454a685a115e\", \"o-nosso-feminism_st_mt.align.json\", 272775)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import time\n",
    "\n",
    "# Helper: yield chunks of size n\n",
    "def chunk_list(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# Get all .align.json files\n",
    "json_folder = \"/content\"\n",
    "json_files = [f for f in os.listdir(json_folder) if f.endswith(\".align.json\")]\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 6  # Try 6 per run\n",
    "\n",
    "# Download first batch only (or change index to run more)\n",
    "for json_file in list(chunk_list(json_files, batch_size))[3]:\n",
    "    path = os.path.join(json_folder, json_file)\n",
    "    print(f\"Downloading {json_file}\")\n",
    "    files.download(path)\n",
    "    time.sleep(1)  # Small delay between downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6tx4nl4Qyyp"
   },
   "outputs": [],
   "source": [
    "!rm -f /content/*.json /content/*.tsv"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Xlp32o8-fuqS"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
